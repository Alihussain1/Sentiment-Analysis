{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "reviews = reviews.lower()\n",
    "reviews = ''.join(char for char in reviews if char not in punctuation)\n",
    "reviews = reviews.split('\\n')\n",
    "\n",
    "##get list of all words\n",
    "allWords = ' '.join(reviews)\n",
    "words = allWords.split()\n",
    "\n",
    "word_counter = Counter(words)\n",
    "sorted_word_counter = sorted(word_counter,key=word_counter.get,reverse=True)\n",
    "word_to_int = {word: indx for indx, word in enumerate(sorted_word_counter,1)}\n",
    "\n",
    "##encoding reviews using word_to_int\n",
    "encoded_reviews = []\n",
    "for rev in reviews:\n",
    "    encoded_reviews.append([word_to_int[word] for word in rev.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n",
      "bromwell  ->  21025\n",
      "high  ->  308\n",
      "Words Count :  6020196\n",
      "Unique Words Count :  74072\n",
      "Encoded Reviews :  25001\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])\n",
    "print(words[0],\" -> \" , word_to_int[words[0]])\n",
    "print(words[1],\" -> \" ,word_to_int[words[1]])\n",
    "print(\"Words Count : \" , len(words))\n",
    "print(\"Unique Words Count : \" , len(word_to_int))\n",
    "print(\"Encoded Reviews : \",len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews:  1\n",
      "Maximum review length:  2514\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Zero-length reviews: \",review_lens[0])\n",
    "print(\"Maximum review length: \",max(review_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Reviews :  25000\n",
      "Encoded Labels :  25000\n"
     ]
    }
   ],
   "source": [
    "#remove the zero length review\n",
    "#must get index to delete same index from labels\n",
    "zero_length_index = [index  for index,encoded_review in enumerate(encoded_reviews) if len(encoded_review) > 0 ]\n",
    "#[ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "encoded_reviews = [encoded_review for encoded_review in encoded_reviews if len(encoded_review) > 0 ]\n",
    "labels = labels.split('\\n')\n",
    "labels = [label for index,label in enumerate(labels) if index  in zero_length_index]\n",
    "encoded_labels = [1 if label=='positive' else 0 for label in labels]\n",
    "print(\"Encoded Reviews : \",len(encoded_reviews))\n",
    "print(\"Encoded Labels : \",len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_reviews[:1])\n",
    "print(encoded_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all reviews same size\n",
    "seq_length = 200 \n",
    "def pad_truncate_features(original_features,length):\n",
    "    #define list of (length) size lists filled with 0\n",
    "    features = np.zeros((len(original_features),length),dtype=int)\n",
    "    #add first (length) integers and pad 0's if len(row) < length\n",
    "    for indx, feature in enumerate(original_features):\n",
    "        features[indx, -len(feature):] = np.array(feature)[:length]\n",
    "    return features\n",
    "        \n",
    "#len(pad_truncate_features(encoded_reviews[:1],seq_length)[0])\n",
    "encoded_reviews = pad_truncate_features(encoded_reviews,seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set :  20000\n",
      "validation set :  2500\n",
      "train set :  2500\n"
     ]
    }
   ],
   "source": [
    "#create train sets, validate sets, test sets\n",
    "split_idx_train = int(len(encoded_reviews)*0.8) # train : 80%  , test & validation 20%\n",
    "split_idx_validate = int(len(encoded_reviews)*0.9) # test : 10% , validation : 10%\n",
    "\n",
    "train_x, test_x, validate_x = encoded_reviews[:split_idx_train], encoded_reviews[split_idx_train:split_idx_validate], encoded_reviews[split_idx_validate:]\n",
    "train_y, test_y, validate_y = encoded_labels[:split_idx_train], encoded_labels[split_idx_train:split_idx_validate], encoded_labels[split_idx_validate:]\n",
    "\n",
    "print(\"train set : \",len(train_x))\n",
    "print(\"validation set : \",len(validate_x))\n",
    "print(\"train set : \",len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "#create tensor dataset then use dataloader on it\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(np.array(train_y)))\n",
    "validate_data = TensorDataset(torch.from_numpy(validate_x),torch.from_numpy(np.array(validate_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(np.array(test_y)))\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "validate_loader = DataLoader(validate_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,num_layers,drop_prob,device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.n_layers = num_layers\n",
    "        self.Embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,hidden_dim,num_layers,batch_first=True,dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.Embeddings(x)\n",
    "        lstm_out, hidden = self.LSTM(x,hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        sig_out = out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GT 650M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (Embeddings): Embedding(74073, 400)\n",
      "  (LSTM): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:102: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n"
     ]
    }
   ],
   "source": [
    "word_size = len(word_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = RNNModel(word_size, embedding_dim, hidden_dim, n_layers,0.5,device)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Validation loss: 0.4642501884698868\n",
      "Epoch: 2   Validation loss: 0.5219473600387573\n",
      "Epoch: 3   Validation loss: 0.525828736424446\n",
      "Epoch: 4   Validation loss: 0.49530929803848267\n",
      "Epoch: 5   Validation loss: 0.5954318666458129\n",
      "Epoch: 6   Validation loss: 0.8499199402332306\n",
      "Epoch: 7   Validation loss: 0.6263167500495911\n",
      "Epoch: 8   Validation loss: 0.793690482378006\n",
      "Epoch: 9   Validation loss: 0.7390493303537369\n",
      "Epoch: 10   Validation loss: 0.8028103798627854\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "clip = 5\n",
    "net.train()\n",
    "iter_counter = 0\n",
    "for e in range(epochs):\n",
    "    #initialize hidden state\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    for batch, labels in train_loader:\n",
    "        iter_counter+=1\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        out, hidden = net.forward(batch,hidden)\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        loss = criterion(out,labels.float())\n",
    "        loss.backward()\n",
    "        #clip gradient to prevent exploding\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    #Validation\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    net.eval()\n",
    "    for v_batch, labels in validate_loader:\n",
    "        v_batch, labels = v_batch.to(device), labels.to(device)\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        out, val_h = net.forward(v_batch,val_h)\n",
    "        loss = criterion(out, labels.float())\n",
    "        val_losses.append(loss.item())\n",
    "    print(\"Epoch: {}\".format(e+1),\n",
    "         \"  Validation loss: {}\".format(np.mean(val_losses)))\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
