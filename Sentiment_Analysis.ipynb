{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "reviews = reviews.lower()\n",
    "reviews = ''.join(char for char in reviews if char not in punctuation)\n",
    "reviews = reviews.split('\\n')\n",
    "\n",
    "##get list of all words\n",
    "allWords = ' '.join(reviews)\n",
    "words = allWords.split()\n",
    "\n",
    "word_counter = Counter(words)\n",
    "sorted_word_counter = sorted(word_counter,key=word_counter.get,reverse=True)\n",
    "word_to_int = {word: indx for indx, word in enumerate(sorted_word_counter,1)}\n",
    "\n",
    "##encoding reviews using word_to_int\n",
    "encoded_reviews = []\n",
    "for rev in reviews:\n",
    "    encoded_reviews.append([word_to_int[word] for word in rev.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:10])\n",
    "print(words[0],\" -> \" , word_to_int[words[0]])\n",
    "print(words[1],\" -> \" ,word_to_int[words[1]])\n",
    "print(\"Words Count : \" , len(words))\n",
    "print(\"Unique Words Count : \" , len(word_to_int))\n",
    "print(\"Encoded Reviews : \",len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Zero-length reviews: \",review_lens[0])\n",
    "print(\"Maximum review length: \",max(review_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the zero length review\n",
    "#must get index to delete same index from labels\n",
    "zero_length_index = [index  for index,encoded_review in enumerate(encoded_reviews) if len(encoded_review) > 0 ]\n",
    "#[ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "encoded_reviews = [encoded_review for encoded_review in encoded_reviews if len(encoded_review) > 0 ]\n",
    "labels = labels.split('\\n')\n",
    "labels = [label for index,label in enumerate(labels) if index  in zero_length_index]\n",
    "encoded_labels = [1 if label=='positive' else 0 for label in labels]\n",
    "print(\"Encoded Reviews : \",len(encoded_reviews))\n",
    "print(\"Encoded Labels : \",len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_reviews[:1])\n",
    "print(encoded_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all reviews same size\n",
    "seq_length = 200 \n",
    "def pad_truncate_features(original_features,length):\n",
    "    #define list of (length) size lists filled with 0\n",
    "    features = np.zeros((len(original_features),length),dtype=int)\n",
    "    #add first (length) integers and pad 0's if len(row) < length\n",
    "    for indx, feature in enumerate(original_features):\n",
    "        features[indx, -len(feature):] = np.array(feature)[:length]\n",
    "    return features\n",
    "        \n",
    "#len(pad_truncate_features(encoded_reviews[:1],seq_length)[0])\n",
    "encoded_reviews = pad_truncate_features(encoded_reviews,seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train sets, validate sets, test sets\n",
    "split_idx_train = int(len(encoded_reviews)*0.8) # train : 80%  , test & validation 20%\n",
    "split_idx_validate = int(len(encoded_reviews)*0.9) # test : 10% , validation : 10%\n",
    "\n",
    "train_x, test_x, validate_x = encoded_reviews[:split_idx_train], encoded_reviews[split_idx_train:split_idx_validate], encoded_reviews[split_idx_validate:]\n",
    "train_y, test_y, validate_y = encoded_labels[:split_idx_train], encoded_labels[split_idx_train:split_idx_validate], encoded_labels[split_idx_validate:]\n",
    "\n",
    "print(\"train set : \",len(train_x))\n",
    "print(\"validation set : \",len(validate_x))\n",
    "print(\"train set : \",len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#create tensor dataset then use dataloader on it\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(np.array(train_y)))\n",
    "validate_data = TensorDataset(torch.from_numpy(validate_x),torch.from_numpy(np.array(validate_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(np.array(test_y)))\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "validate_loader = DataLoader(validate_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,num_layers,drop_prob,device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.n_layers = num_layers\n",
    "        self.Embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,hidden_dim,num_layers,batch_first=True,dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self,x,hidden):\n",
    "        x = self.Embeddings(x)\n",
    "        lstm_out, hidden = self.LSTM(x,hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size = len(word_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = RNNModel(word_size, embedding_dim, hidden_dim, n_layers,0.5,device)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "clip = 5\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    #initialize hidden state\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        out, hidden = net.forward(batch,hidden)\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        loss = criterion(out,labels)\n",
    "        loss.backward()\n",
    "        #clip gradient to prevent exploding\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    #Validation\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    net.eval()\n",
    "    for v_batch, labels in validate_loader:\n",
    "        v_batch, labels = v_batch.to(device), labels.to(device)\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        out, val_h = net.forward(v_batch,val_h)\n",
    "        loss = criterion(out, labels)\n",
    "        val_losses.append(loss.item())\n",
    "    print(\"Epoch: {}\".format(e+1),\n",
    "         \"  Validation loss: {}\".format(np.mean(val_losses)))\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
